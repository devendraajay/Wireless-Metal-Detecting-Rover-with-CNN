Implementation
The implementation of the wireless metal detection rover was carried out in a modular and phase-wise approach. Below is a detailed breakdown of how each component and functionality was practically realized:

1. Hardware Setup and Wiring
a. Motor and Driver Circuit
Component Used: L293N Motor Driver Module

Wiring:

IN1 → GPIO17 (Left Motor Forward)

IN2 → GPIO27 (Left Motor Backward)

IN3 → GPIO22 (Right Motor Forward)

IN4 → GPIO23 (Right Motor Backward)

EN1 (PWM) → GPIO18

EN2 (PWM) → GPIO19

Power Supply: 12V battery, shared between motors and Raspberry Pi using a voltage regulator.

b. Obstacle Sensors
IR Sensors:

Left IR Sensor connected to GPIO20

Right IR Sensor connected to GPIO21

Ultrasonic Sensor:

TRIG → GPIO5

ECHO → GPIO6

c. Camera Integration
Forward-facing Camera: Pi Camera Module used for live streaming to the Flask web interface.

Downward-facing Camera: USB webcam (/dev/video0) used to capture images for mineral detection.

2. Movement Logic and Obstacle Handling
a. GPIO Initialization
The Raspberry Pi was configured in BCM mode using the RPi.GPIO library.

PWM was configured at 1000 Hz for EN1 and EN2 to control motor speed.

b. Obstacle Detection and Handling
IR Sensor: Used for detecting cliffs or sharp edges. If a drop is detected, the rover stops and turns.

Ultrasonic Sensor: Measures forward distance; the rover stops or changes direction if an obstacle is within 10 cm.

c. Movement Functions
Functions were implemented to control movement:

python
Copy
Edit
def move_forward():     # IN1 + IN3 HIGH
def move_backward():    # IN2 + IN4 HIGH
def turn_left():        # IN1 + IN4 HIGH
def turn_right():       # IN2 + IN3 HIGH
def stop():             # All LOW
3. Flask-Based Web Rover Interface
a. Web Control
A Flask-based HTML interface allows real-time control using keyboard inputs (W/A/S/D).

Routes like /forward, /left, /right are mapped to corresponding movement functions.

b. Live Streaming
Pi Camera stream is served using MJPEG over Flask at /video_feed.

Enables user to see forward direction in real-time.

c. Image Capture
A separate route triggers USB webcam to capture a frame.

Captured image is then passed to the CNN model for classification.

4. CNN-Based Mineral Classification
a. Model Training
Model was trained using Keras with TensorFlow backend on 16 mineral classes.

Initially used a basic CNN; later upgraded to EfficientNetB0 for better accuracy.

Preprocessing included normalization, image resizing (128×128), and augmentation.

Final model: only_metals.h5

b. Model Inference
python
Copy
Edit
from tensorflow.keras.models import load_model
model = load_model("only_metals.h5")

def classify(image):
    img = preprocess(image)  # Resize, normalize
    pred = model.predict(img)
    return predicted_label, confidence
c. Image Input
Captured using OpenCV: cv2.VideoCapture(0)

Inference is triggered manually or periodically while the rover moves.

5. Logging and Output
Each detection result is logged with timestamp, filename, predicted mineral, and confidence.

Logged in a results.csv file for further analysis.

Optionally, prediction is displayed on the web UI.

6. System Integration and Testing
a. Final Code Integration
A single app.py Flask file integrates all modules:

Motor control

Camera streaming

CNN inference

USB camera capture

Logging

b. Testing
Indoor Testing: Verified object detection, image capture, movement on tile floors.

Outdoor Testing: Rover was deployed with solar power in semi-autonomous mode.

Manual Mode: Used when obstacle detection fails or terrain is unpredictable.

7. Final Enhancements and Future Scope
Servo-mounted ultrasonic sensor tested for sweep scanning.

CNN model optimization with TensorFlow Lite planned for embedded deployment.

Future upgrades:

A* path planning

Grid mapping

Return-to-start functionality

Remote auto-classification map export

