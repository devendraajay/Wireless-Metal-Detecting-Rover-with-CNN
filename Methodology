Methodology
The proposed system integrates real-time mineral classification using Convolutional Neural Networks (CNNs) with autonomous or manual terrain exploration using a Raspberry Pi-powered rover. The methodology can be divided into several key phases:

1. System Design and Component Integration
Hardware Setup:

Microcontroller: Raspberry Pi 4 (used for motor control, camera input, CNN inference).

Motors: 4 DC motors driven via L293N motor driver.

Obstacle Detection: IR sensors (GPIO20 & GPIO21) and Ultrasonic sensor (TRIG: GPIO5, ECHO: GPIO6) for left, right, and center detection.

Metal Detection Camera: USB webcam (downward-facing) to capture images of the terrain.

Path Planning Camera: Raspberry Pi Camera Module (forward-facing) for manual control view.

Power Supply:

Powered by a 12V, 20W solar panel and battery for autonomous outdoor use.

2. Data Collection and CNN Model Training
Dataset Preparation:

Images of various metallic minerals were collected and labeled into 16 classes including copper, gold, iron, biotite, malachite, etc.

Images resized to 128×128 (initially), then upgraded to 224×224 for improved performance using EfficientNet or ResNet50V2.

CNN Model Training:

Architecture: Three Conv2D layers initially; later replaced with ResNet50V2 and EfficientNetB0.

Preprocessing: Normalization, data augmentation (rotation, zoom, flips), and class balancing.

Training: Models trained using Adam optimizer, categorical crossentropy, early stopping, and learning rate reduction.

Evaluation: Confusion matrix and classification report used to measure accuracy and class-wise performance.

Output: Trained models saved as .h5 (e.g., mineral_classification_model.h5).

3. Real-Time Image Classification
USB Camera Integration:

USB camera mounted downward-facing to capture images on the go.

Triggered manually via Flask interface or automatically at each step.

CNN Inference:

Captured image resized and passed to the CNN model using TensorFlow/Keras.

Predicted class (e.g., "Gold", "Iron") along with confidence score displayed in real-time.

Display & Logging:

Prediction shown on the web interface (optional).

Data logged into CSV or database for later review.

4. Rover Navigation and Control
Motor Control:

GPIO pins used to control L293N motor driver (forward, backward, left, right).

IR and ultrasonic sensors detect nearby obstacles.

Modes of Operation:

Manual Mode: User controls rover using keyboard through a Flask web interface.

Semi-Autonomous Mode: Rover moves in straight lines and avoids obstacles using sensor feedback.

Path Planning Strategy:

Currently zig-zag exploration or straight-line with obstacle-based decision-making.

Future integration: A* or frontier-based autonomous path planning.

5. Web Interface for Control and Streaming
Flask-based Interface:

Live video stream using PiCamera (forward-facing).

Keyboard control for directional movement.

Image capture from USB camera on button click or auto-trigger.

Streaming & Classification:

MJPEG stream for low-latency preview.

Classification displayed on the same UI panel.

6. Logging and Reporting
Prediction Logging:

Each step’s image, timestamp, and predicted mineral type logged to a .csv file.

Final Output:

Logged map with positions of detected minerals (optional heatmap or text report).

Offline analysis possible using prediction logs.

7. Optimization and Deployment
Model Optimization:

Future plans include conversion to TensorFlow Lite or quantized models for onboard inference.

Real-time speed vs accuracy balance tested for embedded deployment.

System Deployment:

Raspberry Pi housed in waterproof casing.

Rover designed for stable operation on rough outdoor terrain.
